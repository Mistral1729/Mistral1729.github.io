<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.1 Transitional//EN">
<html>
	<head>	
		<meta charset="utf-8" />
  		<meta name="generator" content="pandoc" />
  		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  		<meta name="dcterms.date" content="2023-09-13" />
  		<title>Statistics 1</title>
		<link rel="icon" type="image/png" href="/pictures/mpg.png">
		<style>
			code{white-space: pre-wrap;}
				span.smallcaps{font-variant: small-caps;}
				span.underline{text-decoration: underline;}
				div.column{display: inline-block; vertical-align: top; width: 50%;}
				div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
				ul.task-list{list-style: none;}
				div.csl-bib-body { }
				div.csl-entry {
				  clear: both;
				}
				.hanging div.csl-entry {
				  margin-left:2em;
				  text-indent:-2em;
				}
				div.csl-left-margin {
				  min-width:2em;
				  float:left;
				}
				div.csl-right-inline {
				  margin-left:2em;
				  padding-left:1em;
				}
				div.csl-indent {
				  margin-left: 2em;
				}  
		</style>
		<link rel="stylesheet" href="/note_style.css">

		<script type="text/javascript" async src="https://cdn.rawgit.com/mathjax/MathJax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
		</script>

		<script type="text/javascript">
			function toggleSolution(tag) {
				var x = document.getElementById("solution"+tag);
				if (x.style.display === "none") {
					x.style.display = "block";
				} else {
					x.style.display = "none";
				}
			}
		</script>
	</head>

	<body>
		<header id="title-block-header">
			<h1 class="title">Statistics 1</h1>
		</header>
		
		<nav id="TOC" role="doc-toc">
			<h3 id="toc-title">Contents</h3>
			<ol>
				<li>
                    <a href="#stats">Statistics</a>
                    <ol>
                        <li><a href="#stats#def">Basic definitions</a></li>
                        <li><a href="#stats#biv">Bivariate data</a></li>
						<li><a href="#stats#cov">Covariance</a></li>
                        <li><a href="#stats#corrcoeff">Correlation coefficient</a></li>
                    </ol>
                </li>
				<li><a href="#prob">Probability</a></li>
				<li><a href="#bibliography">References</a></li>
			</ol>
		</nav>

		<h2 id="course-content">Course Content</h2>
		<p>
			<b>Statistics</b> <br>
			Bivariate data, Scatter Diagram, Two-way frequency distribution, Marginal frequency distribution, Conditional frequency distribution, Covariance, Simple Correlation, 
			Correlation coefficient, Cauchy Schwartz inequality, Properties of correlation coefficient, Regression analysis, Least square analysis, Regression lines and their 
			properties, Rank data, Rank Correlation, Spearman's Rank Correlation - it's derivation and properties, Spearman's Rank Correlation with perfect agreement, Spearman's 
			Rank Correlation with perfect disagreement. <br>
			<b>Probability</b> <br>
			Random variables, Probability distribution, Cumulative Probability distribution, Probability mass function, Probability density function, Expectations, Mean, Variance, 
			Moment about a point, Raw moments, Central moments, Skewness, Kurtosis, Probability distribution function of two variables. <br>
			<b>Practical</b> <br>
			Calculation of correlation coefficient, Regression and Calculation of Spearman's Rank Correlation.
		</p>

        <h2 id="stats">Statistics</h2>
        <h3 id="stats#def">Basic definitions</h3>
        <p>
			<div class="definition">
                <ol>
					Consider \(n\) pairs of values \(x_i\ \forall i = 1,\ ...,\ n \) of the variable \(x\). Then we define the following
                    <li>\(\displaystyle \bar{x} = \mu = \mathbb{E}[x] := \frac{1}{n} \sum_{i=1}^n x_i\). (Mean, or expectation.) </li>
                    <li>\(\displaystyle \text{Var}(x) := \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2 = \mathbb{E}[(x - \bar{x})^2]\). (Variance.) </li>
                    <li>\(\displaystyle \text{S}_x := \sqrt{\text{Var}(x)} = \sqrt{\frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2} \). (Standard deviation.)</li>
					<li>\(\displaystyle \text{MD}_{\bar{x}} := \mathbb{E}[\ \lvert x - \bar{x} \rvert\ ] = \frac{1}{n} \sum_{i=1}^n \lvert x_i - \bar{x} \rvert \). (Mean deviation about mean.)</li> 
                </ol>
            </div>
			<div class="theorem">
				Standard deviation cannot be less than the mean deviation about mean.
			</div>
			<small><i><a href="#solution#1" class="text" onclick="toggleSolution('#1')">Toggle proof</a></i></small>
			<div id="solution#1" class="proof" style="display: none;">
				Consider \(n\) pairs of values \(x_i\ \forall i = 1,\ ...,\ n \) of the variable \(x\). By definition, 
				\[\displaystyle
					\text{S}_x = \sqrt{\frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2}
				\]
				is the standard deviation of \(x\) and 
				\[\displaystyle
					\text{MD}_{\bar{x}} = \frac{1}{n} \sum_{i=1}^n \lvert x_i - \bar{x} \rvert
				\]
				is the mean deviation about mean of \(x\). We define 
				\[\displaystyle
					a_i = \lvert x_i - \bar{x} \rvert \ \forall i = 1,\ ...,\ n\\
					b_i = 1 \ \forall i = 1,\ ...,\ n
				\]
				then by the Cauchy-Schwarz inequality (Lemma 5), we have 
				\[\displaystyle
					(\sum_{i=1}^n a_{i}^2)(\sum_{i=1}^n b_{i}^2) \geq (\sum_{i=1}^n a_i b_i)^2 \\
					\iff (\sum_{i=1}^n {\lvert x_i - \bar{x} \rvert}^2)(\sum_{i=1}^n 1^2) \geq (\sum_{i=1}^n \lvert x_i - \bar{x} \rvert \cdot 1)^2 \\
					\iff (\sum_{i=1}^n (x_i - \bar{x})^2)n \geq (\sum_{i=1}^n \lvert x_i - \bar{x} \rvert)^2 \\
					\iff \frac{1}{n}(\sum_{i=1}^n (x_i - \bar{x})^2) \geq \frac{1}{n^2}(\sum_{i=1}^n \lvert x_i - \bar{x} \rvert)^2 \\
					\iff \sqrt{\frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2} \geq \frac{1}{n} \sum_{i=1}^n \lvert x_i - \bar{x} \rvert \\
					\iff \text{S}_x \geq \text{MD}_{\bar{x}}.
				\]
			</div>
        </p>

		<h3 id="stats#biv">Bivariate data</h3>
		<p>
			Data that is collected simultaneously for two variables is bivariate. 
			Consider a class of students with their heights and 
			weights collected simultaneously for the variables, say, \(X_a\) and \(Y_a\) 
			respectively, such that, 
			\[\displaystyle
				\begin{array}{ c c }
					X_a & Y_a \\
					x_1 & y_1 \\
					x_2 & y_2
				\end{array}
			\]
			Consider another bivariate data, where the marks of the students in maths and physics are 
			collected simultaneously for the variables, say, \(X\) and \(Y\) respectively. <br>
			In bivariate analysis there are two fundamental types of problems, 
			<ol>
				<li>correlation analysis, and</li>
				<li>regression analysis.</li>
			</ol>
		</p>
		<h4>Correlation analysis</h4>
		<p>
			In correlation analysis we want to study the nature of existence of association (if any) between the variables. 
			Considering the second piece of bivariate data for the class, our aim is to make an association, or <i>correlation</i> 
			between the marks in maths and marks in physics, i.e. a relation between \(X\) and \(Y\).
		</p>
		<h4>Regression analysis</h4>
		<p>
			In regression analysis we want to find express one variable (dependent) as a function of another variable (independent), 
			so that the value of the dependent variable can be predicted whenever the independent variable's value is known. Of course, 
			this is only possible once an association has been established between the variables. Thus regression analysis is only valid 
			when there is a correlation between the variables.
		</p>
		<h4>Scatter or dot diagram</h4>
		<p>
			Scatter diagram is a graphical representation of bivariate data. 
			Suppose we have \(n\) pairs of values \(\{(x_{i}, y_{i})\}_{i=1}^n\). 
			If we plot the points \((x_{i}, y_{i})\) in \(\mathbb{R}^2\) (or the 
			\(2\)-dimensional \(xy\)-plane), the diagram so obtained is called a 
			scatter or dot diagram. <br>
			From a scatter diagram we may study the nature of existence or association 
			between the two variables.
		</p>
		<div class="example">
			<ul>
				<li>
					When there is no clear relation between \(x_i\) and \(y_i\), <br> <br>
					<img src="/documents/notes/stats_1/diagrams/no_corr.png">
				</li>
				<li>
					When there is a linear relation between \(x_i\) and \(y_i\) with positive slope, <br> <br>
					<img src="/documents/notes/stats_1/diagrams/pos_corr.png">
				</li>
				<li>
					When there is a linear relation between \(x_i\) and \(y_i\) with negative slope, <br> <br>
					<img src="/documents/notes/stats_1/diagrams/neg_corr.png">
				</li>
			</ul>
		</div>

        <h3 id="stats#cov">Covariance</h3>
        <p>
            <div class="definition">
                Consider \(n\) pairs of values \(\{(x_{i}, y_{i})\}_{i=1}^n\) of the variables \(x\) and \(y\). 
                Then  
                \[\displaystyle
                    \text{Cov}(x, y) := \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})
                \]
                is the <b>covariance of \(x\) and \(y\)</b>.
            </div>
        </p>

        <h3 id="stats#corrcoeff">Correlation coefficient</h3>
        <p>
            <div class="definition">
                Consider \(n\) pairs of values \(\{(x_{i}, y_{i})\}_{i=1}^n\) of the variables \(x\) and \(y\). 
                Then  
                \[\displaystyle
                    r_{xy} := \frac{\text{Cov}(x, y)}{\sqrt{\text{Var}(x)\text{Var}(y)}}
                \]
                is the <b>correlation coefficient between the variables \(x\) and \(y\)</b>.
            </div>
            <div class="theorem">
                <ol style="list-style: lower-latin;">
                    <li>\(\displaystyle \text{Cov}(x, y) = \frac{1}{n} \sum_{i=1}^n x_i y_i - \bar{x}\bar{y}.\)</li>
                    <li>\(\displaystyle \text{Var}(x) = \frac{1}{n} \sum_{i=1}^n x_i^2 - \bar{x}^2.\)</li>
                    <li>\(\displaystyle \text{Var}(y) = \frac{1}{n} \sum_{i=1}^n y_i^2 - \bar{y}^2.\)</li>
                </ol>
            </div>
			<small><i><a href="#solution#2" class="text" onclick="toggleSolution('#2')">Toggle proof</a></i></small>
			<div id="solution#2" class="proof" style="display: none;"> 
				By definition, 
				\[\displaystyle
					\text{Cov}(x, y) = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) \\
					= \frac{1}{n} \sum_{i=1}^n (x_i y_i - \bar{x} y_i - x_i \bar{y} + \bar{x} \bar{y}) \\
					= \frac{1}{n} (\sum_{i=1}^n x_i y_i - \sum_{i=1}^n \bar{x} y_i - \sum_{i=1}^n x_i \bar{y} + \sum_{i=1}^n \bar{x} \bar{y}) \\
					= \frac{1}{n} (\sum_{i=1}^n x_i y_i - \bar{x} \sum_{i=1}^n y_i - \bar{y} \sum_{i=1}^n x_i + n \bar{x} \bar{y}) \\
					= \frac{1}{n} (\sum_{i=1}^n x_i y_i - \bar{x} n \bar{y} - \bar{y} n \bar{x} + n \bar{x} \bar{y}) \\
					= \frac{1}{n} (\sum_{i=1}^n x_i y_i - n \bar{x} \bar{y}) \\
					= \frac{1}{n} \sum_{i=1}^n x_i y_i - \bar{x}\bar{y}.\ \text{(a)}
				\]
				Again, by definition,
				\[\displaystyle
					\text{Var}(x) = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2 \\
					= \frac{1}{n} \sum_{i=1}^n (x_i^2 - 2 x_i \bar{x} + \bar{x}^2) \\
					= \frac{1}{n} (\sum_{i=1}^n x_i^2 - \sum_{i=1}^n 2 x_i \bar{x} + n \bar{x}^2) \\
					= \frac{1}{n} (\sum_{i=1}^n x_i^2 - 2 \sum_{i=1}^n x_i \bar{x} + n \bar{x}^2) \\
					= \frac{1}{n} (\sum_{i=1}^n x_i^2 - 2 n \bar{x}^2 + n \bar{x}^2) \\
					= \frac{1}{n} (\sum_{i=1}^n x_i^2 - n \bar{x}^2) \\
					= \frac{1}{n} \sum_{i=1}^n x_i^2 - \bar{x}^2.\ \text{(b)}
				\]
				Substituting \(y = x\) in (b) yields (c).	
			</div>

            <div class="theorem">
                Correlation coefficient of any two variables is a dimensionless quantity, <br>
                i.e. it's a pure number, <br>
                i.e. it's independent of any units of measurement.
            </div>
			<small><i><a href="#solution#3" class="text" onclick="toggleSolution('#3')">Toggle proof</a></i></small>
			<div id="solution#3" class="proof" style="display: none;"> 
				Consider the correlation coefficient of n pairs of values \(\{(x_{i}, y_{i})\}_{i=1}^n\) of the variables \(x\) and \(y\). 
				Let the unit of \(x\) be \(L\) and that of \(y\) be \(M\). Then, the unit of \(\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i\) is 
				\(L\) and similarly that of \(\bar{y} = \frac{1}{n} \sum_{i=1}^n y_i\) is \(M\). <br>
				Thus unit of \(\text{Var}(x) = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2\) is \(L^2\), and similarly that of \(\text{Var}(y)\ M^2\). <br>
				Also, unit of \(\text{Cov}(x, y) = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})\) is \(LM\). <br>
				So, for the correlation coefficient of x and y we have the units highlighted in red,
				\[\displaystyle
                    r_{xy} = 
					\frac{
						\overbrace{\text{Cov}(x, y)}^{\color{red}{LM}}
					}
					{
						\underbrace{\sqrt{\text{Var}(x)\text{Var}(y)}}_{\color{red}{\sqrt{L^2 M^2}\ =\ LM}}
					}
                \] 
				\(\therefore\) unit of \(r_{xy}\) is \(\color{red}{\frac{LM}{LM} = L^0 M^0}\).
					
			</div>

            <div class="theorem">
                Correlation coefficient of any two variables is symmetrical w.r.t. the variables, <br>
                i.e. \(r_{xy} = r_{yx}\).
            </div>
			<small><i><a href="#solution#4" class="text" onclick="toggleSolution('#4')">Toggle proof</a></i></small>
			<div id="solution#4" class="proof" style="display: none;">
				By definition, 
				\[\displaystyle
					r_{xy} = \frac{\text{Cov}(x, y)}{\sqrt{\text{Var}(x)\text{Var}(y)}} \\
					= \frac{\frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\text{Var}(y)\text{Var}(x)}} \\
					= \frac{\frac{1}{n} \sum_{i=1}^n (y_i - \bar{y})(x_i - \bar{x})}{\sqrt{\text{Var}(y)\text{Var}(x)}} \\
					= \frac{\text{Cov}(y, x)}{\sqrt{\text{Var}(y)\text{Var}(x)}} \\
					= r_{yx}
				\]
				which proves the theorem.
			</div>

            <div class="theorem">
                The numerical value of the correlation coefficient is independent of the change of origin and 
                scale of the axes.
            </div>
			<small><i><a href="#solution#5" class="text" onclick="toggleSolution('#5')">Toggle proof</a></i></small>
			<div id="solution#5" class="proof" style="display: none;">
				Consider \(n\) pairs of values \(\{(x_{i}, y_{i})\}_{i=1}^n\) of the variables \(x\) and \(y\). 
				In general, any transformation of the origin and scale of the axes from \(x\) and \(y\) to \(u\) 
				and \(v\) will be of the form,
				\[\displaystyle
					u = \frac{(x - a)}{c},\ v = \frac{(y - b)}{d},\ a,\ b,\ c,\ d \in \mathbb{R}\\
					\iff x = c u + a,\ y = d v + b \\
					\therefore\ x_i = c u_i + a,\ y_i = d v_i + b. \\
					\implies \frac{1}{n} \sum_{i=1}^n x_i = \frac{1}{n} \sum_{i=1}^n (c u_i + a),\ 
					\frac{1}{n} \sum_{i=1}^n y_i = \frac{1}{n} \sum_{i=1}^n (d v_i + b)\\ 
					\overbrace{\implies}^{\text{by linearity}} \bar{x} = \frac{c}{n} \sum_{i=1}^n u_i + \frac{1}{n} \sum_{i=1}^n a,\ 
					\bar{y} = \frac{d}{n} \sum_{i=1}^n v_i + \frac{1}{n} \sum_{i=1}^n b \\
					\therefore\ \bar{x} = c \bar{u} + a,\ \bar{y} = d \bar{v} + b \\
				\]
				We also know that, 
				\[\displaystyle
					\text{Var}(x) = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2 \\
					= \frac{1}{n} \sum_{i=1}^n (c u_i + \color{red}{a} - c \bar{u} + \color{red}{a})^2 \\
					= \frac{c^2}{n} \sum_{i=1}^n (u_i - \bar{u})^2 \\
					= c^2 \text{Var}(u).
				\]
				And similarly, \(\text{Var}(y) = d^2 \text{Var}(v)\). Now, 
				\[\displaystyle
					\text{Cov}(x, y) = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) \\
					= \frac{1}{n} \sum_{i=1}^n (c u_i + \color{red}{a} - c \bar{u} + \color{red}{a})(d v_i + \color{red}{b} - d \bar{v} + \color{red}{b}) \\
					= \frac{cd}{n} \sum_{i=1}^n (u_i - \bar{u})(v_i - \bar{v}) \\
					= cd \text{Cov}(u, v).
				\]
				Finally, 
				\[\displaystyle
					r_{xy} = \frac{\text{Cov}(x, y)}{\sqrt{\text{Var}(x)\ \text{Var}(y)}} \\
					= \frac{cd \text{Cov}(u, v)}{\sqrt{c^2 \text{Var}(u)\ d^2 \text{Var}(v)}} \\ 
					= \frac{cd}{|cd|} \frac{\text{Cov}(x, y)}{\sqrt{\text{Var}(x)\ \text{Var}(y)}} \\
					\implies 
					|r_{xy}| = |\frac{cd}{|cd|} \frac{\text{Cov}(x, y)}{\sqrt{\text{Var}(x)\ \text{Var}(y)}}| \\
					= \frac{|cd|}{|cd|} |\frac{\text{Cov}(x, y)}{\sqrt{\text{Var}(x)\ \text{Var}(y)}}| \\
					= |r_{uv}|.
				\]
				This proves the theorem.
			</div>
        </p>

		<h2 id="prob">Probability</h2>

		<h1 class="unnumbered" id="bibliography">References</h1>
		
			<div id="refs" class="references csl-bib-body" role="doc-bibliography">
				<div id="casella-berger" class="csl-entry" role="doc-biblioentry">
                <div class="csl-left-margin">1. </div><div class="csl-right-inline">George Casella, Roger L. Berger, <em>Statistical Inference</em> (Duxbury Advanced Series, 2001).</div>
				<div id="mukhopadhyay" class="csl-entry" role="doc-biblioentry">
                <div class="csl-left-margin">2. </div><div class="csl-right-inline">Nitis Mukhopadhyay, <em>Probability and Statistical Inference</em> (Marcel Dekker AG, 2000).</div>
			</div>
		<hr />
		
			<small>	
				<p class="date">
					Last updated on 13.09.2023.<br>
					<a href="/#wiki">Notes</a>
				</p>
			</small>
	</body>
</html>
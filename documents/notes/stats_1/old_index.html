<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.1 Transitional//EN">
<html>
	<head>	
		<meta charset="utf-8" />
  		<meta name="generator" content="pandoc" />
  		<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  		<meta name="dcterms.date" content="2023-09-13" />
  		<title>Statistics 1</title>
		<link rel="icon" type="image/png" href="/pictures/mpg.png">
		<style>
			code{white-space: pre-wrap;}
				span.smallcaps{font-variant: small-caps;}
				span.underline{text-decoration: underline;}
				div.column{display: inline-block; vertical-align: top; width: 50%;}
				div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
				ul.task-list{list-style: none;}
				div.csl-bib-body { }
				div.csl-entry {
				  clear: both;
				}
				.hanging div.csl-entry {
				  margin-left:2em;
				  text-indent:-2em;
				}
				div.csl-left-margin {
				  min-width:2em;
				  float:left;
				}
				div.csl-right-inline {
				  margin-left:2em;
				  padding-left:1em;
				}
				div.csl-indent {
				  margin-left: 2em;
				}  
		</style>
		<link rel="stylesheet" href="/note_style.css">

		<script type="text/javascript" async src="https://cdn.rawgit.com/mathjax/MathJax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
		</script>

		<link rel="stylesheet" type="text/css" href="http://tikzjax.com/v1/fonts.css">
		<script src="http://tikzjax.com/v1/tikzjax.js"></script>

		<script type="text/javascript">
			function toggleSolution(tag) {
				var x = document.getElementById("solution"+tag);
				if (x.style.display === "none") {
					x.style.display = "block";
				} else {
					x.style.display = "none";
				}
			}
		</script>
	</head>

	<body>
		<header id="title-block-header">
			<h1 class="title">Statistics 1</h1>
		</header>
		
		<nav id="TOC" role="doc-toc">
			<h3 id="toc-title">Contents</h3>
			<ol>
				<li>
					<a href="#prob">Probability</a>
					<ol>
						<li>
							<a href="#prob#3">Probability distributions and densities</a>
							<ol>
								<li><a href="#prob#3#1">Univariate case</a></li>
								<li><a href="#prob#3#2">Bivariate case</a></li>
							</ol>
						</li>
					</ol>
				</li>
				<li>
                    <a href="#stats">Statistics</a>
                    <ol>
                        <li><a href="#stats#def">Basic definitions</a></li>
                        <li><a href="#stats#biv">Bivariate data</a></li>
						<li><a href="#stats#cov">Covariance</a></li>
                        <li><a href="#stats#corrcoeff">Correlation coefficient</a></li>
                    </ol>
                </li>
				<li><a href="#bibliography">References</a></li>
			</ol>
		</nav>

		<h2 id="course-content">Course Content</h2>
		<p>
			<b>Probability</b> <br>
			Random variables, Probability distribution, Cumulative Probability distribution, Probability mass function, Probability density function, Expectations, Mean, Variance, 
			Moment about a point, Raw moments, Central moments, Skewness, Kurtosis, Probability distribution function of two variables. <br>
			<b>Statistics</b> <br>
			Bivariate data, Scatter Diagram, Two-way frequency distribution, Marginal frequency distribution, Conditional frequency distribution, Covariance, Simple Correlation, 
			Correlation coefficient, Cauchy Schwartz inequality, Properties of correlation coefficient, Regression analysis, Least square analysis, Regression lines and their 
			properties, Rank data, Rank Correlation, Spearman's Rank Correlation - it's derivation and properties, Spearman's Rank Correlation with perfect agreement, Spearman's 
			Rank Correlation with perfect disagreement. <br>
			<b>Practical</b> <br>
			Calculation of correlation coefficient, Regression and Calculation of Spearman's Rank Correlation.
		</p>

		<h2 id="prob">Probability</h2>
		<h3 id="prob#3">Probability distributions and densities</h3>
		<p>
			<div class="definition">
				If \(S\) is a sample space with a probability measure and \(X\) is a real-valued function defined over \(S\), then \(X\) is called a <b>random variable</b>.
				If the sample space is finite or countably infinite then X is a discrete random variable, whilst for continuous sample spaces we have continuous random variables.
			</div>

			<h4 id="prob#3#1">Univariate case</h4>
			<div class="definition">
				If \(X\) is a discrete random variable, then the function
				\[
					f(x) = P(X = x) \ \text{for each x within the range of X}
				\]
				is called the <b>probability mass function</b> or probability distribution of \(X\). <br>
				An association such as, 
				<table>
					<tr><td>\(X \)</td><td>\(:\ x_1\)</td><td>\(x_2\)</td><td>...</td><td>\(x_i\)</td><td>...</td></tr>
					<tr><td>\(P(X = x) \)</td><td>\(:\ p_1\)</td><td>\(p_2\)</td><td>...</td><td>\(p_i\)</td><td>...</td></tr>
				</table>
				is called a <b>discrete probability distribution</b> of the random variable \(X\).
			</div>
			
			<div class="theorem">
				If \(X\) is a discrete random variable, then the function \(f(x)\) can serve as the probability distribution of \(X\) iff, 
				<ol>
					<li>\(f(x) \geq 0\) for all values in the function's domain,</li>
					<li>\(\displaystyle \sum_x f(x) = 1, \) where the summation extends over all values within the function's domain.</li>
				</ol>
			</div>
			
			<div class="definition">
				If \(X\) is a discrete random variable, then the function
				\[
					F(x) = P(X \leq x) = \sum_{t \leq x} f(t), \ \text{ for }\ -\infty < x < \infty
				\]
				is called the <b>cumulative probability distribution function</b> or distribution function of \(X\). <br>
				(\(f(t)\) being the probability distribution of \(X\) at \(t\).)
			</div>
			
			<div class="theorem">
				If \(X\) is a discrete random variable, then the following hold for the cumulative probability distribution \(F(X)\),
				<ol>
					<li>\(F(-\infty) = 0\) and \(F(\infty) = 1\),</li>
					<li>\(\forall a, b \in \mathbb{R},\ a < b \implies F(a) \leq F(b) \).</li>
				</ol>
			</div>
			
			<div class="theorem">
				If \(X\) is a discrete random variable with its range consisting of the values 
				\[
					x_1 < x_2 < x_3 < ... < x_n
				\]
				then \(f(x_1) = F(x_1)\) and 
				\[
					f(x_i) = F(x_i) - F(x_{i-1}) \ \text{ for } i \geq 2.
				\]
			</div>
			
			<div class="definition">
				If \(X\) is a continuous random variable and a function \(f(x)\) is defined for all \(x \in \mathbb{R}\), then it is called 
				a <b>probability density function</b> of \(X\) iff 
				\[
					P(a \leq X \leq b) = \int_a^b f(x)dx, \ \text{ for any real constants }\ a\ \text{ and }\ b,\ a \leq b.
				\]
			</div>
			
			<div class="remark"> We have 
				\(P(X = c) = 0\) for any real constant \(c\).
			</div>
			
			<div class="corollary">
				If \(X\) is a continuous random variable and \(a\) and \(b\) are real constants with \(a \leq b\), then 
				\[
					P(a \leq X \leq b) = P(a \leq X < b) = P(a < X \leq b) = P(a < X < b).
				\]
			</div>
			
			<div class="theorem">
				If \(X\) is a continuous random variable, then a function \(f(x)\) can serve as a probability density function of \(X\) iff, 
				<ol>
					<li>\(f(x) \geq 0\ \text{ for }\ -\infty < x < \infty\),</li>
					<li>\(\displaystyle \int_{-\infty}^{\infty} f(x) = 1\).</li>
				</ol>
			</div>
			
			<div class="definition">
				If \(X\) is a continuous random variable and the value of its probability density at \(t\) is \(f(t)\), then the function given by 
				\[
					F(x) = P(X \leq x) = \int_{-\infty}^x f(t)dt, \ \text{ for }\ -\infty < x < \infty
				\]
				is called the <b>cumulative probability distribution function</b> or distribution function of \(X\).
			</div>

			<div class="theorem">
				If \(f(x)\) and \(F(x)\) are the values of the probability density and distribution function of \(X\) at \(x\) respectively, then 
				\[
					P(a \leq X \leq b) = \int_a^b f(x)dx = F(b) - F(a), \ \text{ for any real constants }\ a\ \text{ and }\ b,\ a \leq b,\ \text{ and} \\
					f(x) = \frac{\mathrm{d}F(x)}{\mathrm{d}x}
				\]
				where the derivative exists.
			</div>

			<h4 id="prob#3#2">Bivariate case</h4>
			<div class="definition">
				If \(X\) and \(Y\) are discrete random variables, then the function  
				\[
					f(x, y) = P(X = x, Y = y) \ \text{ for each pair }\ (x,\ y)\ \text{ within the range of X and Y}
				\]
				is called the <b>joint probability mass function</b> or joint probability distribution of \(X\) and \(Y\). <br>
			</div>

			<div class="theorem">
				A bivariate function \(f(x, y)\) can serve as the joint probability distribution of a pair of discrete random variables \(X\) and \(Y\) iff, 
				<ol>
					<li>\(f(x, y) \geq 0\) for all pairs \((x,\ y)\) within the function's domain,</li>
					<li>\(\displaystyle \sum_x \sum_y f(x, y) = 1, \) where the double summation extends over all possible pairs \((x,\ y)\) within the function's domain.</li>
				</ol>
			</div>

			<div class="definition">
				If \(X\) and \(Y\) are discrete random variables, then the function  
				\[
					F(x, y) = P(X \leq x, Y \leq y) = \sum_{s \leq x} \sum_{t \leq y} f(s, t),\ \text{ for }\ -\infty < x < \infty,\ -\infty < x < \infty
				\]
				is called the <b>joint cumulative distribution function</b> or joint distribution function of \(X\) and \(Y\). <br>
			</div>

			<div class="definition">
				A bivariate function \(f(x, y)\) defined over the \(xy\)-plane is called a <b>joint probability density function</b> of the 
				continuous random variables \(X\) and \(Y\) iff, 
				\[
					P[(X, Y) \in A] = \iint_A f(x, y) dx dy
				\]
				for any region \(A\) in the \(xy\)-plane.
			</div>

			<div class="theorem">
				A bivariate function \(f(x, y)\) can serve as the joint probability density function of the continuous random variables \(X\) and \(Y\) iff, 
				<ol>
					<li>\(f(x, y) \geq 0\ \text{ for }\ -\infty < x < \infty, -\infty < y < \infty\),</li>
					<li>\(\displaystyle \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f(x, y) dx dy = 1\).</li>
				</ol>
			</div>

			<div class="definition">
				If \(X\) and \(Y\) are continuous random variables, then the function  
				\[
					F(x, y) = P(X \leq x, Y \leq y) = \int_{-\infty}^{y} \int_{-\infty}^{x} f(s, t) ds dt,\ \text{ for }\ -\infty < x < \infty,\ -\infty < x < \infty
				\]
				is called the <b>joint cumulative distribution function</b> or joint distribution function of \(X\) and \(Y\). <br>
			</div>

			<div class="remark">
				We have 
				\[
					f(x, y) = \frac{\partial^2}{\partial x\partial y} F(x,y)
				\]
				whenever the partial derivatives exist.
			</div>

		</p>

        <h2 id="stats">Statistics</h2>
        <h3 id="stats#def">Basic definitions</h3>
        <p>
			<div class="definition">
                <ol>
					Consider \(n\) values \(\{x_i\}_{i=1}^n \) of the variable \(x\). Then we define the following
                    <li>\(\displaystyle \bar{x} = \mu = \mathbb{E}[x] := \frac{1}{n} \sum_{i=1}^n x_i\). (Mean, or expectation.) </li>
                    <li>\(\displaystyle \text{Var}(x) := \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2 = \mathbb{E}[(x - \bar{x})^2]\). (Variance.) </li>
                    <li>\(\displaystyle \text{S}_x := \sqrt{\text{Var}(x)} = \sqrt{\frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2} \). (Standard deviation.)</li>
					<li>\(\displaystyle \text{MD}_{\bar{x}} := \mathbb{E}[\ \lvert x - \bar{x} \rvert\ ] = \frac{1}{n} \sum_{i=1}^n \lvert x_i - \bar{x} \rvert \). (Mean deviation about mean.)</li> 
                </ol>
            </div>
			<div class="theorem">
				Standard deviation cannot be less than the mean deviation about mean.
			</div>
			<small><i><a href="#solution#1" class="text" onclick="toggleSolution('#1')">Toggle proof</a></i></small>
			<div id="solution#1" class="proof" style="display: none;">
				Consider \(n\) pairs of values \(x_i\ \forall i = 1,\ ...,\ n \) of the variable \(x\). By definition, 
				\[\displaystyle
					\text{S}_x = \sqrt{\frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2}
				\]
				is the standard deviation of \(x\) and 
				\[\displaystyle
					\text{MD}_{\bar{x}} = \frac{1}{n} \sum_{i=1}^n \lvert x_i - \bar{x} \rvert
				\]
				is the mean deviation about mean of \(x\). We define 
				\[\displaystyle
					a_i = \lvert x_i - \bar{x} \rvert \ \forall i = 1,\ ...,\ n\\
					b_i = 1 \ \forall i = 1,\ ...,\ n
				\]
				then by the Cauchy-Schwarz inequality, we have 
				\[\displaystyle
					\left(\sum_{i=1}^n a_{i}^2\right)\left(\sum_{i=1}^n b_{i}^2\right) \geq \left(\sum_{i=1}^n a_i b_i\right)^2 \\
					\iff \left(\sum_{i=1}^n {\lvert x_i - \bar{x} \rvert}^2\right)\left(\sum_{i=1}^n 1^2\right) \geq \left(\sum_{i=1}^n \lvert x_i - \bar{x} \rvert \cdot 1\right)^2 \\
					\iff \left(\sum_{i=1}^n (x_i - \bar{x})^2\right)n \geq \left(\sum_{i=1}^n \lvert x_i - \bar{x} \rvert\right)^2 \\
					\iff \frac{1}{n}\left(\sum_{i=1}^n (x_i - \bar{x})^2\right) \geq \frac{1}{n^2}\left(\sum_{i=1}^n \lvert x_i - \bar{x} \rvert\right)^2 \\
					\iff \sqrt{\frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2} \geq \frac{1}{n} \sum_{i=1}^n \lvert x_i - \bar{x} \rvert \\
					\iff \text{S}_x \geq \text{MD}_{\bar{x}}.
				\]
			</div>
        </p>

		<h3 id="stats#biv">Bivariate data</h3>
		<p>
			Data that is collected simultaneously for two variables is bivariate. 
			Consider a class of students with their heights and 
			weights collected simultaneously for the variables, say, \(X_a\) and \(Y_a\) 
			respectively, such that, 
			\[\displaystyle
				\begin{array}{ c c }
					X_a & Y_a \\
					x_1 & y_1 \\
					x_2 & y_2
				\end{array}
			\]
			Consider another bivariate data, where the marks of the students in maths and physics are 
			collected simultaneously for the variables, say, \(X\) and \(Y\) respectively. <br>
			In bivariate analysis there are two fundamental types of problems, 
			<ol>
				<li>correlation analysis, and</li>
				<li>regression analysis.</li>
			</ol>
		</p>
		<h4>Correlation analysis</h4>
		<p>
			In correlation analysis we want to study the nature of existence of association (if any) between the variables. 
			Considering the second piece of bivariate data for the class, our aim is to make an association, or <i>correlation</i> 
			between the marks in maths and marks in physics, i.e. a relation between \(X\) and \(Y\).
		</p>
		<h4>Regression analysis</h4>
		<p>
			In regression analysis we want to find express one variable (dependent) as a function of another variable (independent), 
			so that the value of the dependent variable can be predicted whenever the independent variable's value is known. Of course, 
			this is only possible once an association has been established between the variables. Thus regression analysis is only valid 
			when there is a correlation between the variables.
		</p>
		<h4>Scatter or dot diagram</h4>
		<p>
			Scatter diagram is a graphical representation of bivariate data. 
			Suppose we have \(n\) pairs of values \(\{(x_{i}, y_{i})\}_{i=1}^n\). 
			If we plot the points \((x_{i}, y_{i})\) in \(\mathbb{R}^2\) (or the 
			\(2\)-dimensional \(xy\)-plane), the diagram so obtained is called a 
			scatter or dot diagram. <br>
			From a scatter diagram we may study the nature of existence or association 
			between the two variables.
		</p>
		<div class="example">
			<ul>
				<li>
					When there is no clear relation between \(x_i\) and \(y_i\), <br> <br>
					<img src="/documents/notes/stats_1/diagrams/no_corr.png">
				</li>
				<li>
					When there is a linear relation between \(x_i\) and \(y_i\) with positive slope, <br> <br>
					<img src="/documents/notes/stats_1/diagrams/pos_corr.png">
				</li>
				<li>
					When there is a linear relation between \(x_i\) and \(y_i\) with negative slope, <br> <br>
					<img src="/documents/notes/stats_1/diagrams/neg_corr.png">
				</li>
			</ul>
		</div>

        <h3 id="stats#cov">Covariance</h3>
        <p>
            <div class="definition">
                Consider \(n\) pairs of values \(\{(x_{i}, y_{i})\}_{i=1}^n\) of the variables \(x\) and \(y\). 
                Then  
                \[\displaystyle
                    \text{Cov}(x, y) := \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})
                \]
                is the <b>covariance of \(x\) and \(y\)</b>.
            </div>
        </p>

        <h3 id="stats#corrcoeff">Correlation coefficient</h3>
        <p>
            <div class="definition">
                Consider \(n\) pairs of values \(\{(x_{i}, y_{i})\}_{i=1}^n\) of the variables \(x\) and \(y\). 
                Then  
                \[\displaystyle
                    r_{xy} := \frac{\text{Cov}(x, y)}{\sqrt{\text{Var}(x)\text{Var}(y)}}
                \]
                is the <b>correlation coefficient between the variables \(x\) and \(y\)</b>.
            </div>
            <div class="theorem">
                <ol style="list-style: lower-latin;">
                    <li>\(\displaystyle \text{Cov}(x, y) = \frac{1}{n} \sum_{i=1}^n x_i y_i - \bar{x}\bar{y}.\)</li>
                    <li>\(\displaystyle \text{Var}(x) = \frac{1}{n} \sum_{i=1}^n x_i^2 - \bar{x}^2.\)</li>
                    <li>\(\displaystyle \text{Var}(y) = \frac{1}{n} \sum_{i=1}^n y_i^2 - \bar{y}^2.\)</li>
                </ol>
            </div>
			<small><i><a href="#solution#2" class="text" onclick="toggleSolution('#2')">Toggle proof</a></i></small>
			<div id="solution#2" class="proof" style="display: none;"> 
				By definition, 
				\[\displaystyle
					\text{Cov}(x, y) = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) \\
					= \frac{1}{n} \sum_{i=1}^n (x_i y_i - \bar{x} y_i - x_i \bar{y} + \bar{x} \bar{y}) \\
					= \frac{1}{n} (\sum_{i=1}^n x_i y_i - \sum_{i=1}^n \bar{x} y_i - \sum_{i=1}^n x_i \bar{y} + \sum_{i=1}^n \bar{x} \bar{y}) \\
					= \frac{1}{n} (\sum_{i=1}^n x_i y_i - \bar{x} \sum_{i=1}^n y_i - \bar{y} \sum_{i=1}^n x_i + n \bar{x} \bar{y}) \\
					= \frac{1}{n} (\sum_{i=1}^n x_i y_i - \bar{x} n \bar{y} - \bar{y} n \bar{x} + n \bar{x} \bar{y}) \\
					= \frac{1}{n} (\sum_{i=1}^n x_i y_i - n \bar{x} \bar{y}) \\
					= \frac{1}{n} \sum_{i=1}^n x_i y_i - \bar{x}\bar{y}.\ \text{(a)}
				\]
				Again, by definition,
				\[\displaystyle
					\text{Var}(x) = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2 \\
					= \frac{1}{n} \sum_{i=1}^n (x_i^2 - 2 x_i \bar{x} + \bar{x}^2) \\
					= \frac{1}{n} (\sum_{i=1}^n x_i^2 - \sum_{i=1}^n 2 x_i \bar{x} + n \bar{x}^2) \\
					= \frac{1}{n} (\sum_{i=1}^n x_i^2 - 2 \sum_{i=1}^n x_i \bar{x} + n \bar{x}^2) \\
					= \frac{1}{n} (\sum_{i=1}^n x_i^2 - 2 n \bar{x}^2 + n \bar{x}^2) \\
					= \frac{1}{n} (\sum_{i=1}^n x_i^2 - n \bar{x}^2) \\
					= \frac{1}{n} \sum_{i=1}^n x_i^2 - \bar{x}^2.\ \text{(b)}
				\]
				Substituting \(y = x\) in (b) yields (c).	
			</div>

            <div class="theorem">
                Correlation coefficient of any two variables is a dimensionless quantity, <br>
                i.e. it's a pure number, <br>
                i.e. it's independent of any units of measurement.
            </div>
			<small><i><a href="#solution#3" class="text" onclick="toggleSolution('#3')">Toggle proof</a></i></small>
			<div id="solution#3" class="proof" style="display: none;"> 
				Consider the correlation coefficient of n pairs of values \(\{(x_{i}, y_{i})\}_{i=1}^n\) of the variables \(x\) and \(y\). 
				Let the unit of \(x\) be \(L\) and that of \(y\) be \(M\). Then, the unit of \(\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i\) is 
				\(L\) and similarly that of \(\bar{y} = \frac{1}{n} \sum_{i=1}^n y_i\) is \(M\). <br>
				Thus unit of \(\text{Var}(x) = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2\) is \(L^2\), and similarly that of \(\text{Var}(y)\ M^2\). <br>
				Also, unit of \(\text{Cov}(x, y) = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})\) is \(LM\). <br>
				So, for the correlation coefficient of x and y we have the units highlighted in red,
				\[\displaystyle
                    r_{xy} = 
					\frac{
						\overbrace{\text{Cov}(x, y)}^{\color{red}{LM}}
					}
					{
						\underbrace{\sqrt{\text{Var}(x)\text{Var}(y)}}_{\color{red}{\sqrt{L^2 M^2}\ =\ LM}}
					}
                \] 
				\(\therefore\) unit of \(r_{xy}\) is \(\color{red}{\frac{LM}{LM} = L^0 M^0}\).
					
			</div>

            <div class="theorem">
                Correlation coefficient of any two variables is symmetrical w.r.t. the variables, <br>
                i.e. \(r_{xy} = r_{yx}\).
            </div>
			<small><i><a href="#solution#4" class="text" onclick="toggleSolution('#4')">Toggle proof</a></i></small>
			<div id="solution#4" class="proof" style="display: none;">
				By definition, 
				\[\displaystyle
					r_{xy} = \frac{\text{Cov}(x, y)}{\sqrt{\text{Var}(x)\text{Var}(y)}} \\
					= \frac{\frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\text{Var}(y)\text{Var}(x)}} \\
					= \frac{\frac{1}{n} \sum_{i=1}^n (y_i - \bar{y})(x_i - \bar{x})}{\sqrt{\text{Var}(y)\text{Var}(x)}} \\
					= \frac{\text{Cov}(y, x)}{\sqrt{\text{Var}(y)\text{Var}(x)}} \\
					= r_{yx}
				\]
				which proves the theorem.
			</div>

            <div class="theorem">
                The numerical value of the correlation coefficient is independent of the change of origin and 
                scale of the axes.
            </div>
			<small><i><a href="#solution#5" class="text" onclick="toggleSolution('#5')">Toggle proof</a></i></small>
			<div id="solution#5" class="proof" style="display: none;">
				Consider \(n\) pairs of values \(\{(x_{i}, y_{i})\}_{i=1}^n\) of the variables \(x\) and \(y\). 
				In general, any transformation of the origin and scale of the axes from \(x\) and \(y\) to \(u\) 
				and \(v\) will be of the form,
				\[\displaystyle
					u = \frac{(x - a)}{c},\ v = \frac{(y - b)}{d},\ a,\ b,\ c,\ d \in \mathbb{R}\\
					\iff x = c u + a,\ y = d v + b \\
					\therefore\ x_i = c u_i + a,\ y_i = d v_i + b. \\
					\implies \frac{1}{n} \sum_{i=1}^n x_i = \frac{1}{n} \sum_{i=1}^n (c u_i + a),\ 
					\frac{1}{n} \sum_{i=1}^n y_i = \frac{1}{n} \sum_{i=1}^n (d v_i + b)\\ 
					\overbrace{\implies}^{\text{by linearity}} \bar{x} = \frac{c}{n} \sum_{i=1}^n u_i + \frac{1}{n} \sum_{i=1}^n a,\ 
					\bar{y} = \frac{d}{n} \sum_{i=1}^n v_i + \frac{1}{n} \sum_{i=1}^n b \\
					\therefore\ \bar{x} = c \bar{u} + a,\ \bar{y} = d \bar{v} + b \\
				\]
				We also know that, 
				\[\displaystyle
					\text{Var}(x) = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2 \\
					= \frac{1}{n} \sum_{i=1}^n (c u_i + \color{red}{a} - c \bar{u} + \color{red}{a})^2 \\
					= \frac{c^2}{n} \sum_{i=1}^n (u_i - \bar{u})^2 \\
					= c^2 \text{Var}(u).
				\]
				And similarly, \(\text{Var}(y) = d^2 \text{Var}(v)\). Now, 
				\[\displaystyle
					\text{Cov}(x, y) = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) \\
					= \frac{1}{n} \sum_{i=1}^n (c u_i + \color{red}{a} - c \bar{u} + \color{red}{a})(d v_i + \color{red}{b} - d \bar{v} + \color{red}{b}) \\
					= \frac{cd}{n} \sum_{i=1}^n (u_i - \bar{u})(v_i - \bar{v}) \\
					= cd \text{Cov}(u, v).
				\]
				Finally, 
				\[\displaystyle
					r_{xy} = \frac{\text{Cov}(x, y)}{\sqrt{\text{Var}(x)\ \text{Var}(y)}} \\
					= \frac{cd \text{Cov}(u, v)}{\sqrt{c^2 \text{Var}(u)\ d^2 \text{Var}(v)}} \\ 
					= \frac{cd}{|cd|} \frac{\text{Cov}(x, y)}{\sqrt{\text{Var}(x)\ \text{Var}(y)}} \\
					\implies 
					|r_{xy}| = |\frac{cd}{|cd|} \frac{\text{Cov}(x, y)}{\sqrt{\text{Var}(x)\ \text{Var}(y)}}| \\
					= \frac{|cd|}{|cd|} |\frac{\text{Cov}(x, y)}{\sqrt{\text{Var}(x)\ \text{Var}(y)}}| \\
					= |r_{uv}|.
				\]
				This proves the theorem.
			</div>
			<div class="proposition">
                The value of the correlation coefficient lies between \(-1\) and \(+1\), inclusive of the endpoints, i.e. 
				\[
					-1 \leq r_{xy} \leq 1.
				\]
            </div>
			<small><i><a href="#solution#6" class="text" onclick="toggleSolution('#6')">Toggle proof</a></i></small>
			<div id="solution#6" class="proof" style="display: none;">
				Consider \(n\) pairs of values \(\{(x_{i}, y_{i})\}_{i=1}^n\) of the variables \(x\) and \(y\). <br>
				Let us define \(a_i := x_i - \bar{x}\) and \(b_i := y_i - \bar{y} \ \forall\ i = 1,...,n\ ;\) then, 
				applying the Cauchy-Schwarz inequality we have, 
				\[\displaystyle
					(\sum_{i=1}^n a_i^2) (\sum_{i=1}^n b_i^2) \geq (\sum_{i=1}^n a_i b_i)^2\\
					\implies (\sum_{i=1}^n (x_i - \bar{x})^2) (\sum_{i=1}^n (y_i - \bar{y})^2) \geq (\sum_{i=1}^n (x_i - \bar{x}) (y_i - \bar{y}))^2\\
					\implies n \text{Var}(x) n \text{Var}(y) \geq (n \text{Cov}(x,\ y))^2\\
					\implies \sqrt{\text{Var}(x) \text{Var}(y)}^2 \geq \text{Cov}(x,\ y)^2\\
					\implies 1 \geq (\frac{\text{Cov}(x,\ y)}{\sqrt{\text{Var}(x) \text{Var}(y)}})^2\\
					\implies 1 \geq r_{xy}^2 = | r_{xy} |^2 \implies | r_{xy} |^2 - 1^2 \leq 0\\
					\implies \underbrace{(| r_{xy} | + 1)}_{\geq\ 1\ >\ 0} (| r_{xy} | - 1) \leq 0\\
					\implies (| r_{xy} | - 1) \leq 0.
				\] 
				Thus, we have \(| r_{xy} | \leq 1 \iff -1 \leq r_{xy} \leq 1.\)
			</div>
        </p>

		<h1 class="unnumbered" id="bibliography">References</h1>
		
			<div id="refs" class="references csl-bib-body" role="doc-bibliography">
				<div id="casella-berger" class="csl-entry" role="doc-biblioentry">
                <div class="csl-left-margin">1. </div><div class="csl-right-inline">George Casella, Roger L. Berger, <em>Statistical Inference</em> (Duxbury Advanced Series, 2001).</div>
				<div id="mukhopadhyay" class="csl-entry" role="doc-biblioentry">
                <div class="csl-left-margin">2. </div><div class="csl-right-inline">Nitis Mukhopadhyay, <em>Probability and Statistical Inference</em> (Marcel Dekker AG, 2000).</div>
			</div>
		<hr />
		
			<small>	
				<p class="date">
					Last updated on 13.09.2023.<br>
					<a href="/#wiki">Notes</a>
				</p>
			</small>
	</body>
</html>